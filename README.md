# Paddle Tennis Movement Feedback System

This project addresses the need for accessible, data-driven feedback for **amateur paddle tennis** players. It builds on research in **motion tracking** and **action recognition** to enhance sports performance through a machine learning-based system that analyzes video-captured body movements and provides real-time, actionable insights.

At the core of the system is a **pose estimation module** that tracks key movementsâ€”such as the _bandeja_ shot. Video data from both professional and amateur players was collected under controlled conditions, and each frame was annotated to map body joint positions. These mappings were categorized based on the quality of the technique and used to train a supervised recursive model. This model evaluates new player movements by comparing them to ideal movement patterns observed in professionals.

A user-friendly digital interface delivers instant feedback and highlights technique deviations, allowing players to make real-time adjustments and foster structured skill development. This system contributes to the fields of **AI**, **computer vision**, and **sports analytics**, providing a practical tool for improving paddle tennis performance.

---

## ğŸ› ï¸ Reproducing the Development Environment

To replicate the development environment on another machine:

1. Clone the repository:  
   ```bash
   git clone https://github.com/CesarEmilioC/thesisRepo_A00830006.git
   cd THESISREPO_A00830006
   ```

2. Create the Conda environment from the YAML file:  
   ```bash
   conda env create -f environment.yml
   ```

3. Activate the environment:  
   ```bash
   conda activate tf1
   ```

4. (Optional) Install additional pip packages:  
   ```bash
   pip install -r requirements.txt
   ```

This ensures that all dependencies and versions used in development are reproduced accurately.

---

## ğŸ“ Project Structure (GitHub Repository)

```bash
THESISREPO_A00830006/
â”‚
â”œâ”€â”€ Coordinates/
â”‚   â”œâ”€â”€ player1/
â”‚   â”œâ”€â”€ player2/
|   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ player9/
â”‚   â””â”€â”€ player10/
â”‚
â”œâ”€â”€ Samples/
â”‚   â”œâ”€â”€ clipSamples/
â”‚   â”‚   â”œâ”€â”€ player10_part1_clip0_grade7.mp4
â”‚   â”‚   â””â”€â”€ player10_part1_clip3_grade8.mp4
â”‚   â”‚
â”‚   â””â”€â”€ coordinateSamples/
â”‚       â”œâ”€â”€ player10_part1_clip0_grade7.json
â”‚       â””â”€â”€ player10_part1_clip3_grade8.json
â”‚
â”œâ”€â”€ Source/
â”‚   â”œâ”€â”€ Models/
â”‚   â”‚   â””â”€â”€ lstm_model.h5
â”‚   â”‚
â”‚   â”œâ”€â”€ Modules/
â”‚   â”‚   â”œâ”€â”€ module_grapher.py
â”‚   â”‚   â”œâ”€â”€ module_LSTM.py
â”‚   â”‚   â””â”€â”€ module_poseEstimation.py
â”‚   â”‚
â”‚   â”œâ”€â”€ openPoseRequirements/
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ environment.yml
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

### Folder Descriptions

- **Coordinates/** â€“ Contains JSON coordinate files generated from OpenPose, organized by player and video part.
- **Samples/** â€“ Stores sample videos and their corresponding JSONs for testing and visualization.  
  - `clipSamples/`: Short video clips of _bandeja_ shots.  
  - `coordinateSamples/`: Corresponding coordinate data for each clip.  
- **Source/Modules/** â€“ Contains Python modules for pose estimation, LSTM model training/testing, and data visualization.
- **Source/Models/** â€“ Stores the trained LSTM model (`lstm_model.h5`).
- **Source/main.py** â€“ CLI entry point that manages pose extraction, analysis, and LSTM operations.
- **environment.yml / requirements.txt** â€“ Environment and dependency definitions.
- **README.md / LICENSE** â€“ Documentation and licensing information.

---

## ğŸ¥ Video Dataset (Independent from GitHub Repository)

The **Videos** folder is hosted independently on OneDrive (not tracked in Git).  
It contains the raw recordings, pre-cut clips organized by player and part, and the JSON timestamp files used to generate clips.

ğŸ”— Dataset link:  
https://tecmx-my.sharepoint.com/:f:/g/personal/a00830006_tec_mx/EuvOsh32lh5El-Aitld6c9UBhsb97xw9q9HbERRJAxOjwg?e=3RdkXB

### Folder Structure (with `part1`, `part2`, ... inside `Clips/`)

```bash
Videos/
â”‚
â”œâ”€â”€ Original Videos/
â”‚   â”œâ”€â”€ player1/
â”‚   â”‚   â”œâ”€â”€ player1_part1.mp4
â”‚   â”‚   â”œâ”€â”€ player1_part2.mp4
â”‚   â”‚   ...
â”‚   â”œâ”€â”€ player2/
â”‚   â”‚   â”œâ”€â”€ player2_part1.mp4
â”‚   â”‚   ...
â”‚   ...
â”‚   â””â”€â”€ player10/
â”‚       â”œâ”€â”€ player10_part1.mp4
â”‚       ...
â”‚
â”œâ”€â”€ Clips/
â”‚   â”œâ”€â”€ player1/
â”‚   â”‚   â”œâ”€â”€ part1/
â”‚   â”‚   â”‚   â”œâ”€â”€ player1_part1_clip1_gradeY.mp4
â”‚   â”‚   â”‚   â”œâ”€â”€ player1_part1_clip2_gradeX.mp4
â”‚   â”‚   â”‚   ...
â”‚   â”‚   â”œâ”€â”€ part2/
â”‚   â”‚   â”‚   â”œâ”€â”€ player1_part2_clip1_gradeY.mp4
â”‚   â”‚   â”‚   ...
â”‚   â”‚   ...
â”‚   â””â”€â”€ player10/
â”‚       â”œâ”€â”€ part1/
â”‚       â”‚   â”œâ”€â”€ player10_part1_clip1_gradeY.mp4
â”‚       â”‚   ...
â”‚       â”œâ”€â”€ part2/
â”‚       â”‚   â”œâ”€â”€ player10_part2_clip1_gradeX.mp4
â”‚       â”‚   ...
â”‚
â”œâ”€â”€ Original Video Cuts/
â”‚   â”œâ”€â”€ player1/
â”‚   â”‚   â”œâ”€â”€ player1_part1.json
â”‚   â”‚   â”œâ”€â”€ player1_part2.json
â”‚   â”‚   ...
â”‚   ...
â”‚   â””â”€â”€ player10/
â”‚       â”œâ”€â”€ player10_part1.json
â”‚       â”œâ”€â”€ player10_part2.json
â”‚       â”œâ”€â”€ player10_part3.json
â”‚       ...
â”‚
â”œâ”€â”€ createClips.py
â””â”€â”€ playerSamples_trainingData.xls
```

---

## ğŸš€ Running the Code

The repository provides a command-line tool (`main.py`) with multiple modules to perform all key operations: pose estimation, visualization, animation, training, prediction, and data validation.

### ğŸ§© General Command Structure

All commands follow the same syntax:

```bash
python main.py <command> [arguments]
```

Run without arguments to display available options:

```bash
python main.py --help
```

---

### ğŸ¯ 1. Pose Estimation with OpenPose

**From a single video:**

```bash
cd Source
python main.py pose --camera "../Samples/clipSamples/player10_part1_clip0_grade7.mp4"
```

**From an entire folder of videos:**

```bash
cd Source
python main.py pose --directory "../Videos/Clips/player10/part1"
```

Arguments:
- `--camera`: Path to a video file or camera index (`0` for webcam).
- `--directory`: Path to a folder containing multiple videos.
- `--model`: OpenPose model type (`mobilenet_thin`, `cmu`, etc.).
- `--show_video`: Display the video during processing.
- `--resize`: Input resolution (e.g., `'432x368'`).

Output: JSON coordinate files saved to the `Coordinates/` folder.

---

### ğŸ“Š 2. Plot Coordinates (Movement Visualization)

Generates trajectory and temporal plots from JSON data.

```bash
cd Source
python main.py plot --file "../Samples/coordinateSamples/player10_part1_clip0_grade7.json" --type all
```

Types available:
- `original`: Raw coordinates.
- `relative`: Relative to the pelvis.
- `temporal`: Joint evolution over time.
- `3d`: 3D spatial trajectory.
- `all`: Generates all plot types.

---

### ğŸï¸ 3. Animate Movement

Creates an animation of a motion sequence using the extracted coordinates.

```bash
cd Source
python main.py animate --file "../Samples/coordinateSamples/player10_part1_clip0_grade7.json"
```

---

### ğŸ§  4. Train LSTM Model

Trains the LSTM network using all player coordinate data.

```bash
cd Source
python main.py trainLSTM --directory "../Coordinates" --model-path "Models/lstm_model.h5"
```

Output: A trained model saved as `lstm_model.h5` inside the `Source/Models/` directory.

#### ğŸ“‚ LSTM Training Results

Every time you train an LSTM model using this command:

```bash
cd Source
python main.py trainLSTM --directory "../Coordinates" --run_name MyExperiment --model_path "Models/lstm_model.h5"
```

A new folder is automatically created inside the Results/ directory using the provided run_name. Each experiment folder stores all training outputs in a fully reproducible format, including:

```bash
Results/
â””â”€â”€ MyExperiment/
    â”œâ”€â”€ training_history.json       â† Loss and accuracy values for each epoch
    â”œâ”€â”€ learning_curves.png         â† Training/validation loss and accuracy plots
    â”œâ”€â”€ confusion_matrix.png        â† Confusion matrix on the test set
    â”œâ”€â”€ class_distribution.png      â† True vs predicted class histogram
    â”œâ”€â”€ classification_report.txt   â† Precision, recall, and F1-score summary
    â””â”€â”€ lstm_model.h5               â† The trained neural network model
```

---

### ğŸ”® 5. Predict Clip Grade (LSTM Inference)

Predicts the quality grade of a clip using the trained LSTM model.

```bash
cd Source
python main.py predictLSTM --file "../Samples/coordinateSamples/player10_part1_clip0_grade7.json" --model-path "Models/lstm_model.h5"
```

Output: Printed predicted grade on the console.

---

### ğŸ“ˆ 6. Count Clips per Grade

Counts how many clips exist per grade label across the dataset.

```bash
cd Source
python main.py countGrades --directory "../Coordinates"
```

---

### ğŸ§© 7. Analyze JSON Validity

Computes the proportion of valid frames (frames where all joints were detected).

```bash
cd Source
python main.py analyzeJSON --directory "../Coordinates"
```

Output: Percentage summary of valid frame data per JSON and the overall mean.

---

## ğŸ‘¤ Authors

**Cesar Emilio CastaÃ±o Marin**  
Thesis Student
Computer Science Masterâ€™s â€“ TecnolÃ³gico de Monterrey  

**Marcial Roberto Leyva FernÃ¡ndez**  
Thesis Advisor  
School of Engineering and Sciences â€“ TecnolÃ³gico de Monterrey